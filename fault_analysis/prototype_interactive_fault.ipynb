{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c57c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "working\n",
    "    look over some examples, want to get the flow and the prompts / messages passed correct\n",
    "\n",
    "to do\n",
    "    examples in the prompts\n",
    "        how to fill out the prompts with incident details and human chose of side\n",
    "        judge and advocate\n",
    "    how to populate to advocate what side they are on\n",
    "    build the graph properly with the conditional edges\n",
    "    how to do the human node\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "want a prototype\n",
    "UI in the webpage.\n",
    "    all relevant details on that page\n",
    "    can take prosecution or defense\n",
    "    button or toggle / switch to argue for that size\n",
    "    \n",
    "will go until user says it is done or 3 turns\n",
    "need a limit. need to only accept valid stuff\n",
    "need an chat window or be able to see text for both sides and then then the verdict\n",
    "I would like langsmith tracing\n",
    "I need to build the graph, have a ui, manage the user prompts\n",
    "the graph:\n",
    "    user goes first. selects prosecution or defense\tor says prosecution or defense in argument \n",
    "        and other side takes opposete.\n",
    "    then other side goes. then user can do a rebuttal or stop. then when user done goes to judge. judge reads the ends.\n",
    "limit on words\n",
    "limit on usage (daily)\n",
    "\n",
    "questions:\n",
    "    how to make sure the judge context is properly annoted ie: this advocate says ... then this advocate says...\n",
    "    how to get context for the ai advocate as well\n",
    "    not sure how to send only parts of the context rather than the whole thing\n",
    "\n",
    "\n",
    "stretch\n",
    "    can share, can save, will be logged and traced locally, can reference images /exhibits / outside documents. \n",
    "    can select order. can select if goes first or not. more models, longer text allowed, user can register\n",
    "    can be the judge and have two AI advocates or no advocates\n",
    "    ai advocate can do repeated rebuttals and consider the human input\n",
    "    graph: opening statements, debate/evidence period, then closing arguments\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f15902",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed, relying on system environment variables\")\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "import argparse\n",
    "import copy \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import sys\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d2e31",
   "metadata": {},
   "source": [
    "### Constants and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "def load_treat_data():\n",
    "   \n",
    "    db_url = \"sqlite:///./avird_data.db\"\n",
    "    \n",
    "    engine = create_engine(db_url)\n",
    "    \n",
    "    # Load data from database\n",
    "    with engine.connect() as conn:\n",
    "        # Get all incidents where ADS was engaged\n",
    "        query = text(\"SELECT * FROM incident_reports WHERE automation_system_engaged = 'ADS'\")\n",
    "        sgo_df = pd.read_sql(query, conn)\n",
    "    \n",
    "    print(f\"Loaded {len(sgo_df)} ADS-engaged incidents from database\")\n",
    "\n",
    "    return sgo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff5ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "\n",
    "# Disable LangSmith tracing for now to avoid API errors\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"AVIRD_fault_v0.0.2\"\n",
    "\n",
    "sgo_df = load_treat_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d5e07",
   "metadata": {},
   "source": [
    "### Set up UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe do a simple gradio example\n",
    "# want prompt for human and what side they are on\n",
    "# text box, submit argument button, then send to judge.\n",
    "# then the past text, next advocate text, then the judge text and final verdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4172a8",
   "metadata": {},
   "source": [
    "### Set Up Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_incident_message(sgo_df, row_index):\n",
    "    # Map column names from current database to original CSV names\n",
    "    check_cols = [\n",
    "        'lighting', 'roadway_type', 'roadway_surface', 'roadway_description', 'posted_speed_limit_mph',\n",
    "        'crash_with', 'weather__clear',\n",
    "        'weather__snow',\n",
    "        'weather__cloudy', \n",
    "        'weather__fog_smoke',\n",
    "        'weather__rain',\n",
    "        'weather__severe_wind',\n",
    "        'weather__unknown',\n",
    "        'weather__other',\n",
    "        'weather__other_text',\n",
    "        'crash_with',\n",
    "\n",
    "        'cp_pre_crash_movement',\n",
    "        'cp_any_air_bags_deployed',\n",
    "        'cp_was_vehicle_towed',\n",
    "        'cp_contact_area__rear_left',\n",
    "        'cp_contact_area__left',\n",
    "        'cp_contact_area__front_left',\n",
    "        'cp_contact_area__rear',\n",
    "        'cp_contact_area__top',\n",
    "        'cp_contact_area__front',\n",
    "        'cp_contact_area__rear_right',\n",
    "        'cp_contact_area__right',\n",
    "        'cp_contact_area__front_right',\n",
    "        'cp_contact_area__bottom',\n",
    "        'cp_contact_area__unknown',\n",
    "        'sv_pre_crash_movement',\n",
    "        'sv_any_air_bags_deployed',\n",
    "        'sv_was_vehicle_towed',\n",
    "        'sv_precrash_speed_mph',\n",
    "        'sv_pre_crash_speed__unknown',\n",
    "        'sv_contact_area__rear_left',\n",
    "        'sv_contact_area__left',\n",
    "        'sv_contact_area__front_left',\n",
    "        'sv_contact_area__rear',\n",
    "        'sv_contact_area__top',\n",
    "        'sv_contact_area__front',\n",
    "        'sv_contact_area__rear_right',\n",
    "        'sv_contact_area__right',\n",
    "        'sv_contact_area__front_right',\n",
    "        'sv_contact_area__bottom',\n",
    "        'sv_contact_area__unknown',\n",
    "        'city',\n",
    "        'state',\n",
    "        'incident_time_2400',\n",
    "        'make',\n",
    "        'model',]\n",
    "\n",
    "    # Get available columns (handle missing columns gracefully)\n",
    "    available_cols = [col for col in check_cols if col in sgo_df.columns]\n",
    "    subset_df = sgo_df[available_cols].iloc[[row_index]].copy()\n",
    "\n",
    "    # Create human-readable column names\n",
    "    column_mapping = {\n",
    "        'make': 'Autonomous Vehicle Make',\n",
    "        'model': 'Autonomous Vehicle Model',\n",
    "        'cp_pre_crash_movement': 'Crash Partner Pre-Crash Movement',\n",
    "        'cp_any_air_bags_deployed': 'Crash Partner Any Air Bags Deployed?',\n",
    "        'cp_was_vehicle_towed': 'Crash Partner Was Vehicle Towed?',\n",
    "        'sv_pre_crash_movement': 'Autonomous Vehicle Pre-Crash Movement',\n",
    "        'sv_any_air_bags_deployed': 'Autonomous Vehicle Any Air Bags Deployed?',\n",
    "        'sv_was_vehicle_towed': 'Autonomous Vehicle Was Vehicle Towed?',\n",
    "        'sv_precrash_speed_mph': 'Autonomous Vehicle Precrash Speed (MPH)',\n",
    "        'incident_time_2400': 'Incident Time (24:00)',\n",
    "        'posted_speed_limit_mph': 'Posted Speed Limit (MPH)',\n",
    "    }\n",
    "    \n",
    "    subset_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "    narrative = sgo_df['narrative'].iloc[row_index] if 'narrative' in sgo_df.columns else \"No narrative available\"\n",
    "\n",
    "    incident_message = f'''\n",
    "        Incident Information:\n",
    "        - Narrative:\n",
    "        {narrative}\n",
    "\n",
    "        - Other Information:\n",
    "    '''\n",
    "\n",
    "    for k, v in subset_df.iloc[0].to_dict().items():\n",
    "        if v is None or pd.isnull(v) or (isinstance(v, str) and v.strip() == '') or \\\n",
    "            (isinstance(v, float) and np.isnan(v)):\n",
    "            continue\n",
    "\n",
    "        if v == 'Y':\n",
    "            treated_v = 'Yes'\n",
    "        elif v == 'N':\n",
    "            treated_v = 'No'\n",
    "        else:\n",
    "            treated_v = copy.deepcopy(v)\n",
    "        incident_message += f'{k}: {treated_v}\\n'\n",
    "    \n",
    "    return incident_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a000d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_ai_advocate_or_judge(state: MessagesState) -> Literal[\"ai_advocate\", \"judge\"]:\n",
    "    '''\n",
    "    Determine to route to judge or ai advocate\n",
    "    If human advocate selects judgement,\n",
    "    '''\n",
    "    if state[\"messages\"][-1].content.lower() == \"start_judgement\":\n",
    "        return \"judge\"\n",
    "    else:\n",
    "        return \"ai_advocate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_interactive_fault_graph(incident_df, row_index=0):\n",
    "\n",
    "    incident_message = get_incident_message(incident_df, row_index)\n",
    "            \n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "        #model_name=\"gpt-4o\"\n",
    "        #model_name=\"gpt-3.5-turbo\"\n",
    "        #o1, o1 mini\n",
    "        # temperature, max tokens\n",
    "        )\n",
    "\n",
    "    AI_ADVOCATE_SYSTEM_PROMPT = (\n",
    "       \"You are an advocate arguing if an autonomous vehicle (AV) is at fault for an incident. \"\n",
    "       \"Another advocate is also arguing on the other side. \"\n",
    "       \"Both arguments and details of the incident will be provided to a judge. \"\n",
    "       \"The judge will decide if the autonomous vehicle is at fault and what percentage of fault the autonomous vehicle has. \"\n",
    "       \"You will be given details of the incident and the other advocate's argument. \\n\"\n",
    "       f\"Here are the details of the incident: \\n\\n {incident_message} \\n\\n\"\n",
    "    )\n",
    "    \n",
    "    JUDGE_SYSTEM_PROMPT = (\n",
    "        \"You are a judge. \"\n",
    "        \"You will be given details of an autonomous vehicle incident and the arguments of two advocates. \"\n",
    "        \"One advocate is arguing that the autonomous vehicle is at fault and the other is arguing that \"\n",
    "        \"the autonomous vehicle is not at fault. \"\n",
    "        \"You will decide if the autonomous vehicle is at fault and what percentage of fault the autonomous vehicle has. \"\n",
    "        \"Briefly explain your reasoning. \\n\"\n",
    "        f\"Here are the details of the incident: \\n\\n {incident_message} \\n\\n\"\n",
    "        \"Here are the arguments of the first advocate: \\n\\n {human_advocate_message} \\n\\n\"\n",
    "        \"Here are the arguments of the second advocate: \\n\\n {ai_advocate_message} \\n\\n\"\n",
    "        \"Rebuttal from the first advocate: \\n\\n {human_advocate_rebuttal} \\n\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    # ai_advocate_prompt = ChatPromptTemplate.from_messages([\n",
    "    #     (\"system\", ai_advocate_system_prompt),\n",
    "    #     MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    # ])\n",
    "\n",
    "    # judge_advocate_prompt = ChatPromptTemplate.from_messages([\n",
    "    #     (\"system\", judge_system_prompt),\n",
    "    #     MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    # ])\n",
    "\n",
    "    workflow = StateGraph(MessagesState)\n",
    "\n",
    "    #ai_advocate_chain = ai_advocate_prompt | llm\n",
    "\n",
    "    #judge_advocate_chain = judge_advocate_prompt | llm\n",
    "\n",
    "    def human_advocate_node(state: MessagesState) -> MessagesState:\n",
    "        # Use invoke instead of ainvoke for synchronous operation\n",
    "        #return {\"messages\": [arbitrator_chain.invoke(state[\"messages\"])]}\n",
    "        pass\n",
    "\n",
    "\n",
    "    def ai_advocate_node(state: MessagesState) -> MessagesState:\n",
    "        prompt = AI_ADVOCATE_SYSTEM_PROMPT.format(incident_message=incident_message)\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        return {\"messages\": [{\"role\": \"advocate_1\", \"content\": response.content}]}\n",
    "\n",
    "\n",
    "    def judge_node(state: MessagesState) -> MessagesState:\n",
    "\n",
    "        human_advocate_message = state[\"messages\"][-3].content\n",
    "        ai_advocate_message = state[\"messages\"][-2].content\n",
    "        human_advocate_rebuttal = state[\"messages\"][-1].content\n",
    "\n",
    "        prompt = JUDGE_SYSTEM_PROMPT.format(incident_message=incident_message,\n",
    "            human_advocate_message=human_advocate_message,\n",
    "            ai_advocate_message=ai_advocate_message,\n",
    "            human_advocate_rebuttal=human_advocate_rebuttal)\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        return {\"messages\": [{\"role\": \"judge\", \"content\": response.content}]}\n",
    "\n",
    "\n",
    "    workflow.add_node(\"human_advocate\", human_advocate_node)\n",
    "    workflow.add_node(\"ai_advocate\", ai_advocate_node)\n",
    "    workflow.add_node(\"judge\", judge_node)\n",
    "\n",
    "    workflow.add_edge(START, \"human_advocate\")\n",
    "\n",
    "    # Conditional edge from human_advocate to routing tool that chooses ai_advocate or judge\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_advocate\",\n",
    "        route_ai_advocate_or_judge,\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"judge\", END)\n",
    "\n",
    "    return workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57767fe2",
   "metadata": {},
   "source": [
    "### Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af890ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the UI\n",
    "# get the incident details\n",
    "# build the graph\n",
    "# other set up\n",
    "\n",
    "# human enters text and hits submit. maybe text check.\n",
    "# kicks off the graph until the next human interaction.\n",
    "# for final interaction, if text box is not empty then it is an additional rebuttal to the dudge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae5317",
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_graph = build_interactive_fault_graph(sgo_df, 0)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(fault_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fault_graph(incident_df, row_index):\n",
    "    '''\n",
    "    run prototype fault graph\n",
    "    '''\n",
    "    # build graph\n",
    "    fault_graph = build_interactive_fault_graph(incident_df, row_index)\n",
    "    # print incident message so user can see it\n",
    "    incident_message = get_incident_message(incident_df, row_index)\n",
    "\n",
    "    print(incident_message)\n",
    "    # get user argument\n",
    "    user_input = input()\n",
    "\n",
    "    if user_input is not None:\n",
    "\n",
    "        for event in fault_graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "            for value in event.values():\n",
    "                print(\"Graph value:\", value[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avird_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
